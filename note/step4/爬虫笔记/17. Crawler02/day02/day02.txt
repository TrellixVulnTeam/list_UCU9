数据结构：
	数组，链表；
	队列，栈；
	哈希表；
	树，二叉树，完全二叉树，红黑树，B+树；
	图；

算法：
	递归--》循环，DP(Dynamical Programming)；空间换时间；
	HASH算法，非对称RSA算法；
	广度优先遍历，深度优先遍历等； 
        快速排序（递归），归并排序（递归）；

组合数：
	从n个不同元素中，任取m(m≤n)个元素并成一组，叫做从n个不同元素中取出m个元素的一个组合；从n个不同元素中取出m(m≤n)个元素的所有组合的个数，叫做从n个不同元素中取出m个元素的组合数。

C(n,m) = n!/(m!(n-m)!)
n! = 1*2*3*...*n
n! = n*(n-1)!
def F(n):
   if n == 1:
     return 1 
   return n*F(n-1)

C(n,m)=C(n-1,m-1)+C(n-1,m)

怎么解决递归中重复运算所带来的空间和时间上的浪费？怎么将已经算法方法保存下来，并且以便很方便的查询出已经计算的结果；这里可以使用DP（动态规划）

https://www.douban.com/doulist/3516235/?start=0&sort=seq&sub_type=

<div class="title">[\s\S]*?>([\s\S]*?)</a>[\s\S]*?<span class="rating_nums">([\s\S]*?)</span>


<div class="bd doulist-subject">
<div class="source">
      来自：豆瓣电影
    </div>
<div class="post">
<a href="https://movie.douban.com/subject/26686170/" target="_blank">
<img src="https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2335656403.jpg" width="100"/>
</a>
</div>
<div class="title">
<a href="https://movie.douban.com/subject/26686170/" target="_blank">
        谎言大爆炸
      </a>
</div>
<div class="rating">
<span class="allstar25"></span>
<span class="rating_nums">4.7</span>
<span>(1057人评价)</span>
</div>
<div class="abstract">
      
          导演: 侯亮
            <br/>
          主演: 孙坚 / 白冰 / 潘霜霜
            <br/>
          类型: 喜剧 / 悬疑
            <br/>
          制片国家/地区: 中国大陆
            <br/>
          年份: 2016
    </div>
</div>

数据分析过程中，有可能出现数据不全的情况，怎么办？
   1）可以给个默认值，补全；
   2）可以使用机器学习的方法填补信息；比如用姓名的信息来推测性别；
   3）删除此条信息；

面试题：
	有两个机房：两个机房里放了一模一样的机器。其中，有一个机房有一台机器坏了，怎么快速的找到这台机器，并且将其数据恢复。
有两个List:
    [1,3,5,7,2,4,6,8]
    [1,5,7,4,2,8,6]
如果是两台机器坏了，怎么办？如果有N台机器坏了，怎么办？
    1+2+3+...+X = sum
              X = sum-(1+2+3+...)

    1+2+3+...+X+Y = sum
 	*2*3....*X*Y = mult
  
去重处理：
 这里有两个队列，一个是待爬队列，既要进也要出，一旦这个待爬队列中没有元素可以出了，说明当前这个爬虫任务完成了；一个是已经爬取的队列，这个队列的元素是只进不出的,只是一个历史记录；
  有两个部分的去重，第一次发生已经爬取的队列中；第二次发生在待爬的队列中；
去重的一层保险：
    关系型数据库中，通过把URL(做hash sha256等)设置成主键，key来最终去重；

关键的十行代码，所表达的含义：
    # 两步去重操作
    crawl_queue = []    # 待爬队列
    crawled_queue = []  # 已爬取队列
    for item in itemUrls:
        if item not in crawled_queue: 
            # 第一步去重，确定这些url不在已爬队列中
            crawl_queue.append(item)
    #第二步去重，对待爬队列去重
    crawl_queue = list(set(crawl_queue))
    
    # 模拟广度优先遍历
    while crawl_queue: #去待爬队列中取值，直到待爬队列为空
        url = crawl_queue.pop(0)#取出待爬队列中第一个值
        CrawlMovieInfo(url)
        # 把已经处理完的url放入已经爬取的队列中
        crawled_queue.append(url)

作业）完善豆瓣爬虫程序的CrawlMovieInfo方法，用一个更合理的逻辑去完善这个程序的数据抓取过程。

